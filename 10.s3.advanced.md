## Section 13 - Amazon S3 (Advanced)  

### S3 Lifecycle Rules - Moving between storage classes  
- You can transition objects between storage classes  
- for infrequently accessed objects, move them to `Standard IA`  
- for archive objects, move them to `Glacier` or `Glacier Deep Archive`  
- Moving objects can be automated using `Lifecycle Rules`  
- Permutations:  
  - `Standard` -> `standard IA` -> `Intelligent Tiering` -> `Glacier Instant Retrieval` -> `Glacier Flexible Retrieval` -> `Glacier Deep Archive`  
  - The previous Storage class can go to any storage class on the right, by skipping straight to it  
  - An exception is One-Zone IA - it sits to the right of Intelligent Tiering but can only go to Glacier Flexible Retrieval or Glacier Deep Archive  

### S3 Lifecycle Rules - Detailed  
- `Transition Actions`
  - transition objects to another storage class  
    - move objects to `Standard IA` after 60 days after creation  
    - move objects to glacier for archiving after 6 months    
- `Expiration Actions`  
  - configure objects to `expire` (be deleted) after a specific amount of time  
    - access logs can be set to delete after 365 days  
    - `can be used to delete old versions of files` - if versioning is enabled  
    - can be used to delete incomplete multi-part uploads after a certain amount of time has passed  
- Rules can be created for a certain `prefix` - eg s3://mybucket/music/*  
- Rules can be created for certain `objects tags` - eg Department: Finance  

### S3 Lifecycle Rules - Scenario 1  
- App on EC2 creates image thumbnails after profile photos are uploaded to S3. 
- Thumbnails can be easily recreated  
- Thumbnails only need to be kept for 60 days  
- source image needs to be immediately retrieved up to 60 days, then the user can wait up to 6 hours  
- `Design`  
  - S3 source images can be on `Standard` with a Lifecycle configuration to transition to `Glacier` after 60 days  
  - S3 thumbnails can be `One-Zone IA` with a lifecycle configuration to `expire` them after 60 days  

### S3 Lifecycle Rules - Scenario 2  
- Company states that you need to recover deleted items immediately for 30 days  
- after this time, recovery for should be up to 48 hours for up to 365 days  
- `Design`  
  - Enable `S3 versioning`    
  - Transition to the "noncurrent" versions of the object to `Standard IA`  
  - Then transition the "noncurrent" versions to `Glacier Deep Archive`  

### Amazon S3 - Analytics (Storage Class Analysis)  
- Helps you decide when to transition objects to the right storage class  
- recommendations for `Standard` and `Standard IA`  
  - does **NOT** work for One-Zone IA and Glacier  
  - report is updated daily  
  - 24 to 48 hours to start seeing the data analysis  
  - Good first step to put together Lifecycle Rules and start on improving on them  

### Amazon S3 - Requester Pays  
- In general, bucket owners pay for all S3 storage and data transfer costs associated with their bucket    
- `EXAM TIP`: with `Requester Pays buckets` the requester pays for the `transfer` of the data from the bucket - they pay for the Network costs!  
- helpful when you want to share large datasets with other accounts  
- the requester **MUST** be authenitcated in AWS - can't be an anonymous user  

### Amazon S3 - Event Notifications  
- `Events include`:  
  - S3:ObjectCreated  
  - S3:ObjectRemoved  
  - S3:ObjectRestore  
  - S3:Replication  
  - ...and more  
- You can filter these events on things like the object name filtering is possible - eg *.jpg  
- Use Case: generate thumbnails of images uploaded to S3  
- `EXAM TIP`: **Can create as many S3 events as desired**  
- S3 event notification typically delivers events in seconds but can sometimes take a minute or longer  
- Events can trigger, things like lambda, or send an SNS or a message to SQS  

### ### Amazon S3 - Event Notifications - IAM Permissions  
- You need to set up IAM access policies depending on what you want to do with the events  
  - `SNS` - create an SNS Resource Access Policy - allows the S3 service to send data into the SNS topic   
  - `SQS` - create an SQS Resource Access Policy - allows S3 service to send data into the SQS queue  
  - `lambda` - create a lambda Resource Policy - allows S3 service to invoke a lambda function   
- You don't use IAM roles, instead you create resource access policies on the SNS topic, SQS Queue or the lambda function  

```  
{
  "Version": "2012-10-17",
  "Statement": {
    "Effect": "Allow",
    "Action": SQS:SendMessage", 
    "Principal": {
      "Service": "s3.amazonaws.com"
    },
    "Resource": "arn:aws:sqs:us-east-1:987654321:MyQueue",
    "Condition": {
      "ArnLike":{
          "aws:SourceArn": "arn:aws:s3:::MyBucket"
      }
    }
  }
}
```

### Amazon S3 - Event Notifications with Amazon EventBridge  
- `ALL` the events end up in `Amazon EventBridge`  
  - You need to set up the integration under Amazon EventBridge, not Event notifications on the S3 bucket  
  - You can then set up rules to send these events to a multitude of different AWS service destinations  
- `Advanced Filtering`  
  - option with JSON rules (metadata, object size, name ...)  
- `Multiple Destinations`  
  - Step Functions, Kinesis Streams or Kinesis Firehose  
- `EventBridge Capabilities`  
  - Archive, Replay Events, Reliable Delivery  

### Amazon S3 - Performance  
- Amazon S3 automatically scales to high request rates, latency 100-200ms  
- `EXAM TIP`: Your application can achieve:  
  - at least `3500 PUT/COPY/POST/DELETE` requests
  - or `5500 GET/HEAD` requests  
  - per second/per prefix in a bucket  
- There is no limit to the amount of prefixes you can have in a bucket  
  - Example: how to obtain the prefix  
    - The prefix is anything between `bucket` and `file`  
    - (object path -> prefix):  
    - bucket/folder1/sub1/file -> /folder1/sub1  
    - bucket/folder1/sub2/file -> /folder1/sub2  
    - bucket/1/file -> /1/  
    - bucket/2/file -> /2/  
- If you spread reads across all four of the above prefixes evenly, you can achieve (5500 x 4) 22,000 requests per second for GET and HEAD  
- `Multi-Part Upload`  
  - recommended for files > 100MB  
  - must use for files > 5GB  
  - can help parallelise uploads - which speeds up transfers  
- `S3 Transfer Acceleration`  
  - Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region  
  - This is compatible with multi-part uploads  
- `S3 Byte-Range Fetches`  
  - Parallelise GETs by requesting specific byte ranges  
  - better resilience in case of failures  
  - `Can be used to speed up downloads`  
  - `Can be used to retreive only partial data - eg, the head of a file`  

### Amazon S3 - S3 Select & Glacier Select  
- Uses SQL statments for simple filtering  
- Retrieve less data using SQL by performing `server-side` filtering  
- can filter by rows and columns (simple SQL statements)  
- less network transfer, less CPU cost on the client side  
- Up to 400% faster and 80% cheaper  
- example, if you want a dataset from an excel file, Select can filter and only send the dataset that you need, reducing the size and cost  

### Amazon S3 - S3 Batch Operations  
- Perform bulk operations on existing S3 objects with a single request  
- Use cases:  
  - modify object metadata & properties   
  - copy objects between S3 buckets  
  - `EXAM TIP`: **Encrypt un-encrypted objectt**  
  - Modify ACLs and tags  
  - restore objects from S3 Glacier  
  - invoke lambda functions to perform custom actions  
- A job consists of:  
  - A `list of objects`  
  - The `action` to take  
  - Optional `parameters`  
- S3 Batch operations manages retries, tracks progress, sends completion notifications, generate reports ...  
- `EXAM TIP`: You can use `S3 inventory` to get a **list** of objects and use `S3 Select` to **filter** the object list  
