## Section 12 - Amazon S3  
- It is one of the main building blocks of AWS  
- S3 is `infinitely scaling` storage  
- Many web sites and AWS Services use S3 as part of their integration.  

### Amazon S3 - Use Cases  
- Backup and Storage  
- DR  
- Archive  
- Hybrid cloud storage  
- Application hosting  
- Media Hosting  
- Data Lakes and big data analytics  
- software delivery  
- static websites  

### Amazon S3 - Buckets  
- S3 allows people to store __objects__ (files)  in **buckets** (directories)  
- Buckets must have a **globally unique name**  
- Buckets are defined at the **regional level**  
- S3 looks like a global service but the buckets are created in a region  
- `Naming convention`:  
  - no uppercase letters or underscores  
  - 3 - 63 characters long
  - can't use an IP  
  - must start with a lowercase letter or number  
  - must NOT start with the prefix `xn--`, `sthree-` or `sthree-configurator`   
  - must not end with the suffix `-s3alias` or `--ol-s3`  

### Amazon S3 - Objects  
- Objects (files) have a `key`  
- The `key` is the Full path:  
  - eg, `s3://my-bucket/my_file.txt`  
- The `key` is composed of a `prefix` + an `object_name`  
  - eg, `s3://my-bucket/` + `my_file.txt`  
- There is no concept of `directories` within buckets, although the UI displays them as such  
- The are just keys with very long names and contain `/`  
- `Object` values are the content of the body:  
  - maximum object size is 5TB (or 5000GB)  
  - if uploading more than 5GB, you must do a `multi-part upload`  
- metadata (list of text key-value pairs - system or user metadata)  
- Tags (unicode key-value pairs - up to 10) - useful for security/lifecycle  
- Version ID (if versioning is available)  
**Note:**  
- Once you create an object in a bucket and try to view it (if it's not public) via the public URL, you will get an `authorisation error` **BUT** if you try to **open** it from the web console, it will give you a `presigned URL`, which is basically a URL with your security token in the link.  


### Amazon S3 - Security  
- **User Based**  
  - `IAM Policies` - which API calls should be allowed for a specific user from IAM  
- **Resource Based**  
  - `Bucket Policies` - bucket wide rules from the S3 console - allows cross account access  
  - `Object Access Control List (ACL)` - finer grain control (can be disabled)  
  - `Bucket Access Control List (ACL)` - less common (can be disabled)  
- The most common way to set up permissions on a bucket is through **Bucket Policies**  
- **Note:**  
- An IAM principal can access an S3 Object if:  
  - The IAM permissions **ALLOW** it **OR** the resource policy **ALLOWS** it  
    **AND** there's no explicit **DENY** rule   
- **Encryption**  
- Encrypt objects in S3 using encryption keys    

### S3 - Bucket Policies  
- JSON based policies  
  - **Resource**: Which buckets and objects this policies applies to eg `arn:aws:s3:::examplebucket/*`  
  - **Effect**: This is ALLOW or DENY of the defined `Action`    
  - **Actions**: The set of API to Allow or Deny, eg `s3:GetObject`  
  - **Principal**: The account or user to apply the policy to, eg, `*`  
- `Use S3 bucket policy to`:  
  - Grant public access to the bucket  
  - Force objects to be encrypted at upload  
  - Grant access to another account (Cross Account)  
- `Additional settings for Blocking Public Access`  
  - Created by AWS as an extra layer of security to prevent company data leaks  
  - Can be set at the **account level**  
  - If the bucket should **NEVER** be public, then leave the following on:  
      - `Block all public access` (leave all these settings on)  
          - granted through new access control lists  
          - granted through any access control lists  
          - granted through new public bucket or access point policies  
          - block cross-account acceess to buckets and objects through any public bucket or access point policies  
          - If all buckets should never be public, you can set this at the account level  
- **Note:**  
- `Making objects public`  
  - First step is to make the bucket public. Go to **Permissions** tab, edit `Block public access` and turn it __off__ to allow public access.  
  - Second step is to create a bucket policy, **edit** it and add one via the policy generator to set one up. 
  - After the second step, you will get a warning about objects being made public!  
- If you get a **403 Forbidden** error, make sure the bucket policy allows public reads.  

### Amazon S3 - Static Website Hosting  
- S3 can host static websites and have them accessible on the internet  
- The webiste URL will be (depending on the region used)  
  - `http://bucket-name.s3-website-aws-region.amazonaws.com`  
  - `http://bucket-name.s3-website.aws-region.amazonaws.com`  
- This is enabled on the bucket itself, you need to navigate to the `Properties` tab and then down to **Static Website Hosting** and select `Enable`
- Select the index document (homepage)  
- Also ensure that you have made all the content in the bucket publically readable.  
- ...and finally add a bucket policy to allow public read access as follows:  
```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicReadGetObject",
            "Effect": "Allow",
            "Principal": "*",
            "Action": [
                "s3:GetObject"
            ],
            "Resource": [
                "arn:aws:s3:::Bucket-Name/*"
            ]
        }
    ]
}
```

### S3 - Versioning  
- You can version your files in S3  
- `It has to be enabled at the bucket level`  
- Same key, overwrite will change the version, that is, 1,2 or 3 etc  
- It's recommended to "version" your buckets  
  - Protect against unintended deletes (because you can restore to a version)  
  - Easy roll back to previous version  
- **Notes:**  
  - Any file that isn't versioned (but already exists) prior to enabling versioning will have version `null`  
    - You will be able to see this when you click the `Versions` tab on the object or `Show Version` in the bucket view
  - `Suspending versioning doesn't delete the previous versions`  
- To show the versions in the Object explorer, flick the radio button that says `Show Versions` (it will only appear if you have selected to enable versioning at some point).    
- `Deleting Objects`  
  - To revert to a previous version, ensure `Show Versions` is enabled, select the new object -> `Delete` it, thus, going back to an older version.  
  - To delete an object, without `Show versions`, it will actually add a **delete marker** and therefore this can be undone. You can see the marker if you `Show versions`.    
  - To get the object back after you have deleted it (added the delete marker), you need to **delete** the **delete marker** through `Show versions`.  
  - **NOTE:** It's best to play around with this feature as it isn't exactly intuitive.  

### S3 - Replication  
- Must have `versioning enabled` in source and destination buckets
- `CRR (Cross-Region Replication)`  
- `SRR (Same-Region Replication)`  
- Buckets can be in different AWS accounts  
- Copying is asynchronous  
- `Use Cases:`  
  - `CRR` - compliance, lower latency access, replication across accounts.  
  - `SRR` - log aggregation, live replication between production and test accounts.  
- After you enable replication, only new objects are replicated  
- Optionally, you can replicate objects using `S3 Batch Replication`  
  - replicates exisiting objects and objects that have failed replication  
- For `DELETE` operations:  
  - Can replicate delete markers from source to the target - but it is an optional setting  
  - Deletions with a version ID are not replicated to avoid malicious deletes from one bucket to the other  
- `NOTE`: Permanent deletes are also not replicated! If you permanently delete a file in the origin bucket, it will not be deleted in the replicated bucket!  
- `There is no chaining of replication`  
  - If bucket 1 has replication into bucket 2, which then has replication into bucket 3  
  - the objects created in bucket 1 are not replicated to bucket 3  
- To set up replication, go to the **origin** bucket, then click on `Management` tab.  
  - Now we need to `Create replication rule` 
  - Select the destination Bucket  
  - Under `IAM role`, select `Create new role` 
    - This will also create an IAM policy for this new role to allow the buckets the proper permissions   
  - `Create` the rule  
  - **NOTE:** It will pop up and ask you if you want to copy across existing objects using a one-time batch process. Select whatever you want here but be aware that it doesn't copy existing content unless you tell it to!  

### Storage Classes  
- Standard  
  - Standard - Infrequent Access  
  - One-Zone - Infrequent Access  
- Glacier  
  - Glacier Instant Retreival  
  - Glacier Flexible Retreival  
  - Glacier Deep Archive   
- Intelligent Tiering  
- Once you create a storage class, you can move between them manually or using S3 lifecycle configurations  
- `Durability and Availability`**`  
  - `Durability`:  
    - High durability 99.999999999% (11 9's) of objects across multiple AZ  
    - If you store 10,000,000 objects with Amazon S3, you can, on average expect to incur a loss of a single object once every 10,000 years!  
    - Same for all storage classes  
  - `Availability`:  
    - Measures how readily available a service is  
    - varies depending on the storage class  
    - eg, s3 standard has 99.99% availability or 53 minutes a year of downtime.  
- `**`S3 Standard - General Purpose`**`  
  - 99.99% Availability  
  - Used for frequently accessed data  
  - Low latency and high throughput  
  - sustains 2 concurrent facility failures  
  - Use Case:  
    - Big Data analytics, mobile & gaming apps, content distribution  
- `S3 Standard - Infrequent Access`  
  - data that is infrequently accessed but still requires rapid access when needed  
  - Lower cost than S3 standard  
  - But you pay for retrieval  
  - `Standard - Infrequent Access (S3 Standard IA)`  
    - 99.9% Availability  
    - Use Case:  
      - DR, backups
  - `One-Zone - Infrequent Access (S3 One Zone IA)`  
    - High Durability (11 9's) in a single AZ; data lost if AZ is destroyed  
    - 99.95% Availability  
    - Use Case:  
      - Secondary backups of on-prem data or data you can re-create  
- `S3 Glacier Storage Classes`
  - Low-cost object storage meant for backups and archiving  
  - pay for storage plus retreival costs  
  - `Glacier Instant Retreival`  
    - ms retrieval, good for data accessed once per quarter  
    - min storage duration of 90 days  
  - `Glacier Flexible Retreival - formerly Amazon S3 Glacier`
    - retrieval tiers: expedited (1 to 5 mins), standard (3 to 5 hours), Bulk (5 to 12 hours but is free)  
    - min storage duration of 90 days  
  - `Glacier Deep Archive`   
    - retrieval tiers: Standard (12 hours), Bulk (48 hours)  
    - min storage duration of 180 days 
- `Intelligent Tiering`  
  - Small monthly monitoring and auto-tiering fee  
  - Moves objects automatically between Access Tiers based on usage  
  - No retrieval charges  
  - `Frequent Access tier (auto)`: default  
  - `Infrequent Access tier (auto)`: object not accessed for 30 days  
  - `Archive Instant Access tier (auto)`: object not accessed for 90 days  
  - `Archive Access tier (optional)`: configurable from 90 to 700+ days  
  - `Deep Archive Access tier (optional)`: configurable from 180 to 700+ days  
- **You select the tier when you upload files to S3**  
- **You can also change the storage class on files**  
- `Lifecycle Management Rules`  
- To set up rules to manage your data movement between storage classes.  
  - Go to the bucket and then select the `Management` tab  
  - Click `Create Lifecycle Rule`  
    - You can limit the scope using filters, or apply to all objects in the bucket  
    - Select the rule actions - eg, move current versions of objects between storage classes after a certain amount of days   
    - Create the rule!  

### S3 Encryption  
- **Server Side Encryption (Default)**  
  - When it arrives in the bucket, Amazon S3 encrypts it  
- **Client Side Encryption**  
  - The user encrypts the file before uploading it to S3  

### IAM Access Analyser for S3  
- Ensures that only intended people have access to your S3 bucket by evaluating:  
  - S3 Bucket Policies  
  - S3 ACLs  
  - S3 Access Point Policies  
- This is powered by the IAM Access Analyser  

### Shared Responsibility Model for S3  
- AWS:  
  - Infra (security, durability, availability, sustain concurrent loss of two facilities)  
  - configuration and vuln analysis  
  - compliance validation  
- Customer:  
  - Versioning  
  - Bucket Policies  
  - Logging and Monitoring  
  - Storage Classes are optimally set  
  - Encryption at rest and in-transit  

## Summary  
`Buckets vs Objects`:  global unique names, ties to a region  
`S3 Security`: IAM Policy, S3 Bucket Policy (Public Access), S3 Encryption  
`S3 Websites`: host a static website  
`S3 Versioning`: multiple version for files, prevent accidental deletion  
`S3 Replication`: same-regions or cross-region, must enable versioning for it to work  
`S3 Storage Classes`: Standard, IA, 1Z-IA, Intelligent, Glacier(Instant, Flexible and Deep)  